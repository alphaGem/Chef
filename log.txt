====================== Initialization ======================
rank :          0
local_rank :    0
world_size :    1
local_size :    1
master :        thunlp-215-5:56893
device :        0
cpus :          [4, 5, 6, 7, 10, 11, 12, 13, 36, 37, 38, 39,
                 42, 43, 44, 45]

load from cache: /home/chenyanxu/.cache/model_center/gpt2-base/config.json
load from cache: /home/chenyanxu/.cache/model_center/gpt2-large/config.json
load from cache: /home/chenyanxu/.cache/model_center/gpt2-base/pytorch_model.pt
load from cache: /home/chenyanxu/.cache/model_center/gpt2-large/pytorch_model.pt
 GPT2(
  (encoder): Encoder(
    (layers): TransformerBlockList(
      (0): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): CheckpointBlock(
        (self_att): SelfAttentionBlock(
          (layernorm_before_attention): LayerNorm()
          (self_attention): Attention(
            (project_q): Linear()
            (project_k): Linear()
            (project_v): Linear()
            (attention_out): Linear()
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ffn): FFNBlock(
          (layernorm_before_ffn): LayerNorm()
          (ffn): FeedForward(
            (w_in): DenseACT(
              (w): Linear()
              (act): GELU()
            )
            (w_out): Linear()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (output_layernorm): LayerNorm()
  )
  (embed_dropout): Dropout(p=0.1, inplace=False)
  (input_embedding): Embedding()
  (position_embedding): Embedding()
)
encoder Encoder(
  (layers): TransformerBlockList(
    (0): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (6): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (7): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (8): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (9): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (10): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (11): CheckpointBlock(
      (self_att): SelfAttentionBlock(
        (layernorm_before_attention): LayerNorm()
        (self_attention): Attention(
          (project_q): Linear()
          (project_k): Linear()
          (project_v): Linear()
          (attention_out): Linear()
          (attention_dropout): Dropout(p=0.1, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ffn): FFNBlock(
        (layernorm_before_ffn): LayerNorm()
        (ffn): FeedForward(
          (w_in): DenseACT(
            (w): Linear()
            (act): GELU()
          )
          (w_out): Linear()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layernorm): LayerNorm()
)
encoder.layers TransformerBlockList(
  (0): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (1): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (2): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (3): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (4): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (5): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (6): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (7): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (8): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (9): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (10): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (11): CheckpointBlock(
    (self_att): SelfAttentionBlock(
      (layernorm_before_attention): LayerNorm()
      (self_attention): Attention(
        (project_q): Linear()
        (project_k): Linear()
        (project_v): Linear()
        (attention_out): Linear()
        (attention_dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ffn): FFNBlock(
      (layernorm_before_ffn): LayerNorm()
      (ffn): FeedForward(
        (w_in): DenseACT(
          (w): Linear()
          (act): GELU()
        )
        (w_out): Linear()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)
encoder.layers.0 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.0.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.0.self_att.layernorm_before_attention LayerNorm()
encoder.layers.0.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.0.self_att.self_attention.project_q Linear()
encoder.layers.0.self_att.self_attention.project_k Linear()
encoder.layers.0.self_att.self_attention.project_v Linear()
encoder.layers.0.self_att.self_attention.attention_out Linear()
encoder.layers.0.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.0.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.0.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.0.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.0.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.0.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.0.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.0.ffn.ffn.w_in.w Linear()
encoder.layers.0.ffn.ffn.w_in.act GELU()
encoder.layers.0.ffn.ffn.w_out Linear()
encoder.layers.0.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.1 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.1.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.1.self_att.layernorm_before_attention LayerNorm()
encoder.layers.1.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.1.self_att.self_attention.project_q Linear()
encoder.layers.1.self_att.self_attention.project_k Linear()
encoder.layers.1.self_att.self_attention.project_v Linear()
encoder.layers.1.self_att.self_attention.attention_out Linear()
encoder.layers.1.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.1.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.1.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.1.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.1.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.1.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.1.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.1.ffn.ffn.w_in.w Linear()
encoder.layers.1.ffn.ffn.w_in.act GELU()
encoder.layers.1.ffn.ffn.w_out Linear()
encoder.layers.1.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.2 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.2.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.2.self_att.layernorm_before_attention LayerNorm()
encoder.layers.2.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.2.self_att.self_attention.project_q Linear()
encoder.layers.2.self_att.self_attention.project_k Linear()
encoder.layers.2.self_att.self_attention.project_v Linear()
encoder.layers.2.self_att.self_attention.attention_out Linear()
encoder.layers.2.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.2.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.2.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.2.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.2.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.2.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.2.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.2.ffn.ffn.w_in.w Linear()
encoder.layers.2.ffn.ffn.w_in.act GELU()
encoder.layers.2.ffn.ffn.w_out Linear()
encoder.layers.2.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.3 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.3.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.3.self_att.layernorm_before_attention LayerNorm()
encoder.layers.3.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.3.self_att.self_attention.project_q Linear()
encoder.layers.3.self_att.self_attention.project_k Linear()
encoder.layers.3.self_att.self_attention.project_v Linear()
encoder.layers.3.self_att.self_attention.attention_out Linear()
encoder.layers.3.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.3.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.3.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.3.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.3.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.3.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.3.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.3.ffn.ffn.w_in.w Linear()
encoder.layers.3.ffn.ffn.w_in.act GELU()
encoder.layers.3.ffn.ffn.w_out Linear()
encoder.layers.3.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.4 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.4.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.4.self_att.layernorm_before_attention LayerNorm()
encoder.layers.4.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.4.self_att.self_attention.project_q Linear()
encoder.layers.4.self_att.self_attention.project_k Linear()
encoder.layers.4.self_att.self_attention.project_v Linear()
encoder.layers.4.self_att.self_attention.attention_out Linear()
encoder.layers.4.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.4.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.4.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.4.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.4.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.4.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.4.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.4.ffn.ffn.w_in.w Linear()
encoder.layers.4.ffn.ffn.w_in.act GELU()
encoder.layers.4.ffn.ffn.w_out Linear()
encoder.layers.4.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.5 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.5.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.5.self_att.layernorm_before_attention LayerNorm()
encoder.layers.5.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.5.self_att.self_attention.project_q Linear()
encoder.layers.5.self_att.self_attention.project_k Linear()
encoder.layers.5.self_att.self_attention.project_v Linear()
encoder.layers.5.self_att.self_attention.attention_out Linear()
encoder.layers.5.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.5.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.5.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.5.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.5.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.5.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.5.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.5.ffn.ffn.w_in.w Linear()
encoder.layers.5.ffn.ffn.w_in.act GELU()
encoder.layers.5.ffn.ffn.w_out Linear()
encoder.layers.5.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.6 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.6.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.6.self_att.layernorm_before_attention LayerNorm()
encoder.layers.6.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.6.self_att.self_attention.project_q Linear()
encoder.layers.6.self_att.self_attention.project_k Linear()
encoder.layers.6.self_att.self_attention.project_v Linear()
encoder.layers.6.self_att.self_attention.attention_out Linear()
encoder.layers.6.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.6.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.6.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.6.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.6.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.6.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.6.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.6.ffn.ffn.w_in.w Linear()
encoder.layers.6.ffn.ffn.w_in.act GELU()
encoder.layers.6.ffn.ffn.w_out Linear()
encoder.layers.6.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.7 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.7.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.7.self_att.layernorm_before_attention LayerNorm()
encoder.layers.7.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.7.self_att.self_attention.project_q Linear()
encoder.layers.7.self_att.self_attention.project_k Linear()
encoder.layers.7.self_att.self_attention.project_v Linear()
encoder.layers.7.self_att.self_attention.attention_out Linear()
encoder.layers.7.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.7.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.7.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.7.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.7.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.7.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.7.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.7.ffn.ffn.w_in.w Linear()
encoder.layers.7.ffn.ffn.w_in.act GELU()
encoder.layers.7.ffn.ffn.w_out Linear()
encoder.layers.7.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.8 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.8.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.8.self_att.layernorm_before_attention LayerNorm()
encoder.layers.8.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.8.self_att.self_attention.project_q Linear()
encoder.layers.8.self_att.self_attention.project_k Linear()
encoder.layers.8.self_att.self_attention.project_v Linear()
encoder.layers.8.self_att.self_attention.attention_out Linear()
encoder.layers.8.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.8.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.8.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.8.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.8.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.8.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.8.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.8.ffn.ffn.w_in.w Linear()
encoder.layers.8.ffn.ffn.w_in.act GELU()
encoder.layers.8.ffn.ffn.w_out Linear()
encoder.layers.8.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.9 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.9.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.9.self_att.layernorm_before_attention LayerNorm()
encoder.layers.9.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.9.self_att.self_attention.project_q Linear()
encoder.layers.9.self_att.self_attention.project_k Linear()
encoder.layers.9.self_att.self_attention.project_v Linear()
encoder.layers.9.self_att.self_attention.attention_out Linear()
encoder.layers.9.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.9.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.9.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.9.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.9.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.9.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.9.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.9.ffn.ffn.w_in.w Linear()
encoder.layers.9.ffn.ffn.w_in.act GELU()
encoder.layers.9.ffn.ffn.w_out Linear()
encoder.layers.9.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.10 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.10.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.10.self_att.layernorm_before_attention LayerNorm()
encoder.layers.10.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.10.self_att.self_attention.project_q Linear()
encoder.layers.10.self_att.self_attention.project_k Linear()
encoder.layers.10.self_att.self_attention.project_v Linear()
encoder.layers.10.self_att.self_attention.attention_out Linear()
encoder.layers.10.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.10.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.10.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.10.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.10.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.10.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.10.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.10.ffn.ffn.w_in.w Linear()
encoder.layers.10.ffn.ffn.w_in.act GELU()
encoder.layers.10.ffn.ffn.w_out Linear()
encoder.layers.10.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.layers.11 TransformerBlock(
  (self_att): SelfAttentionBlock(
    (layernorm_before_attention): LayerNorm()
    (self_attention): Attention(
      (project_q): Linear()
      (project_k): Linear()
      (project_v): Linear()
      (attention_out): Linear()
      (attention_dropout): Dropout(p=0.1, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ffn): FFNBlock(
    (layernorm_before_ffn): LayerNorm()
    (ffn): FeedForward(
      (w_in): DenseACT(
        (w): Linear()
        (act): GELU()
      )
      (w_out): Linear()
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
encoder.layers.11.self_att SelfAttentionBlock(
  (layernorm_before_attention): LayerNorm()
  (self_attention): Attention(
    (project_q): Linear()
    (project_k): Linear()
    (project_v): Linear()
    (attention_out): Linear()
    (attention_dropout): Dropout(p=0.1, inplace=False)
    (softmax): Softmax(dim=-1)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.11.self_att.layernorm_before_attention LayerNorm()
encoder.layers.11.self_att.self_attention Attention(
  (project_q): Linear()
  (project_k): Linear()
  (project_v): Linear()
  (attention_out): Linear()
  (attention_dropout): Dropout(p=0.1, inplace=False)
  (softmax): Softmax(dim=-1)
)
encoder.layers.11.self_att.self_attention.project_q Linear()
encoder.layers.11.self_att.self_attention.project_k Linear()
encoder.layers.11.self_att.self_attention.project_v Linear()
encoder.layers.11.self_att.self_attention.attention_out Linear()
encoder.layers.11.self_att.self_attention.attention_dropout Dropout(p=0.1, inplace=False)
encoder.layers.11.self_att.self_attention.softmax Softmax(dim=-1)
encoder.layers.11.self_att.dropout Dropout(p=0.1, inplace=False)
encoder.layers.11.ffn FFNBlock(
  (layernorm_before_ffn): LayerNorm()
  (ffn): FeedForward(
    (w_in): DenseACT(
      (w): Linear()
      (act): GELU()
    )
    (w_out): Linear()
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
encoder.layers.11.ffn.layernorm_before_ffn LayerNorm()
encoder.layers.11.ffn.ffn FeedForward(
  (w_in): DenseACT(
    (w): Linear()
    (act): GELU()
  )
  (w_out): Linear()
)
encoder.layers.11.ffn.ffn.w_in DenseACT(
  (w): Linear()
  (act): GELU()
)
encoder.layers.11.ffn.ffn.w_in.w Linear()
encoder.layers.11.ffn.ffn.w_in.act GELU()
encoder.layers.11.ffn.ffn.w_out Linear()
encoder.layers.11.ffn.dropout Dropout(p=0.1, inplace=False)
encoder.output_layernorm LayerNorm()
embed_dropout Dropout(p=0.1, inplace=False)
input_embedding Embedding()
position_embedding Embedding()
